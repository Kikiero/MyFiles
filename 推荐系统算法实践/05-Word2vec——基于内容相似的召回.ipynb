{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "第5章\tWord2vec——基于内容相似的召回\n",
    "5.1\tWord2vec算法\n",
    "5.1.1\t语言模型\n",
    "5.1.2\tCBOWOne-WordContext模型\n",
    "5.1.3\tCBOWMulti-WordContext模型\n",
    "5.1.4\tSkip-Gram模型\n",
    "5.1.5\tHierarchicalSoftmax\n",
    "5.1.6\tNegativeSampling\n",
    "5.2\tWord2vec实例\n",
    "5.2.1\tSpark实现\n",
    "5.2.2\tTensorFlow实现"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "内容相似就是通过对物品内容的理解，比如物品的属性、物品的特征等，得到物品的向量表达，然后通过向量Hi金呢的相似度计算得相似物品列表。\n",
    "基于文本内容的物品，常用Word2vec方法计算相似内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2013年谷歌开源词向量计算工具-Word2Vec。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语言模型"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "语言模型也就是NLP自然语言处理，比如分词、信息提取、命名实体识别、词性标注、句法分析、语音识别、语音合成、机器翻译、问答系统、信息检索等。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "语言模型是对一段文本的出现概率进行估计的数学表达式，是一个长度为T的字符串所确定的概率分布P(w1,w2,.,wT)，表示其存在的可能性，其中W1.Wt表示这段文本中的各个词。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "语言模型可以对一段文本的概率进行估计，对信息检索，机器翻译，语音识别等任务有着重要的作用。语言模型分为统计语言模型和神经网络语言模型。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p(s)\n",
    "\n",
    "=p(w1​,w2​,……,wT​)\n",
    "\n",
    "=p(w1​)∗p(w2​∣w1​)∗……∗p(wT​∣w1​,w2​,……,wT−1​)\n",
    "\n",
    "∑p(wi​∣Contenti​)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "语音识别到两个句子，“我喜欢打篮球”“我喜欢打足球”，通过语言模型计算这两个句子出现的概率：\n",
    "p(我喜欢打篮球)\n",
    "=p(我)*p(喜欢|我)*p(打|我，喜欢)*p(篮球|我，喜欢，打)\n",
    "p(我喜欢打足球)\n",
    "=p(我)∗p(喜欢∣我)∗p(打∣我，喜欢)∗p(足球∣我，喜欢，打)\n",
    "p(我喜欢打足球)=p(我)∗p(喜欢∣我)∗p(打∣我，喜欢)∗p(足球∣我，喜欢，打)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec是语言模型中的一种，它是一种从大量文本语料中以无监督方式学习语义知识的模型，他被大量的用于自然语言处理中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec有两个模型，如果一个词作为输入来预测它周围的上下文，这个模型就是Skip-Gram模型。而拿一个词的上下文作为输入来预测这个词本身，则是CBOW模型。Word2vec不是一个模型，是一个工具，包含两个模型。一个是跳字模型一个是词袋模型。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "这里我的学习去搜了B站视频\n",
    "https://www.bilibili.com/video/BV12W411v7Ga?from=search&seid=17641354178786593871\n",
    "【10 mins】：词向量和word2vec概述。\n",
    "【15 mins】：跳字模型。\n",
    "【15 mins】：连续词袋模型。\n",
    "【10 mins】：负采样。\n",
    "【10 mins】：层序softmax。\n",
    "\n",
    "这里发现了两个资源地址：\n",
    "1、IT资源汇总地址：https://github.com/XiangLinPro/IT_book\n",
    "2、《动手学深度学习》https://zh.d2l.ai/chapter_how-to-use/how-to-use.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我们希望向量的表示方法可以提供一些有意义的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 有两套比较高效的训练方法，一种是负采样一种是层序softmax。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "word2ve可以较好的表达不同词之间的相似和类比关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 跳字模型"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "跳字模型，用一个词来预测它在文本序列周围的词。\n",
    "Skip-Gram模型会根据中心单词预测其上下文信息。\n",
    "模型是一个3层神经网络模型，包含了输入层、隐含层和输出层，\n",
    "输入层只1个词向量，输出层有C个词向量。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "例如，给定 the man hit his son\n",
    "这个模型是给定hit 生成the man his son的概率。\n",
    "hit叫做中心词\n",
    "the man his son叫做背景词。\n",
    "由于hit只生成2个距离，窗口大小为2."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "生成概率：\n",
    "wt是t时刻的词\n",
    "生成t+j时刻的词\n",
    "j范围-m到m的范围\n",
    "生成\n",
    "w t-2\n",
    "w t-1\n",
    "w t+1\n",
    "w t+2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我们用中心词生成背景词，每一个词是独立的。\n",
    "我们希望所有文本的中心词我们都做一遍。\n",
    "生成一个联合概率。\n",
    "最大化联合概率。\n",
    "联合概率最大化=最小化-（1/t）*log\n",
    "通过找到这个词在中心词和背景词的向量表达，来最小化这个损失函数。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "这个向量表达式是P。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./src/0501.png\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"./src/0501.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "V 中心词的向量表达式\n",
    "U 背景词的向量表达式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词袋模型"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "拿一个词的上下文作为输入来预测这个词语本身。CBOW模型。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CBOW One-Word Context，这个模型是假定context只有一个词。\n",
    "也就是上下文信息只有一个词。\n",
    "该网络模型有3层：输入层、隐含层、输出层。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CBOW Multi-Word Context模型，利用多个上下文单词来推中心单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 有两套比较高效的训练方法，一种是负采样一种是层序softmax。¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 层序Softmax-Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "层序Softmax是一种有效计算softmax的方式。\n",
    "这个模型使用一颗二叉树来表示词汇表中的所有单词。\n",
    "所有的单词都在二叉树的叶节点上。\n",
    "对于每个叶节点从根节点到叶节点只有一条路径，这条路径用来评估该叶节点上单词的出现概率值。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "原始Softmax复杂度O(n)，n是词典大小。\n",
    "哈夫曼树，复杂度可以降为O(h),h为树高度。因为只需要预测从根节点到相应叶节点的路径。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "哈夫曼树的构造会根据词的词频，逐渐合并两个小词频，从下往上合并至根节点。词频较大的离根节点较近，相应的哈夫曼编码也会很短。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 负采样 Negative Sampling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在每次迭代过程中，有大量的输出向量需要更新，负采样的思想是每次只更新其中一部分输出向量。\n",
    "最终需要输出的上下文单词（正样本）在采样过程中应该保留下来并更新，同事需要采集一些单词作为负样本，这样就将多分类问题变成二分类问题。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在采样过程中，可以任意选择一种概率分布。\n",
    "这种概率分布叫做噪声分布。\n",
    "一般来说，物品出现频次越高，对应的index越小。\n",
    "index越大，被采样到的概率越小，\n",
    "物品越热门，越有可能成为负样，选中高频词的概率较大，选中低频词的概率较小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
