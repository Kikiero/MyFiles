{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "第6章\t逻辑回归\n",
    "6.1\t逻辑回归算法\n",
    "6.1.1\t二元逻辑回归模型\n",
    "6.1.2\t模型参数估计\n",
    "6.1.3\t多元逻辑回归模型（Softmax回归）\n",
    "6.1.4\t逻辑回归的网络结构\n",
    "6.1.5\t梯度下降算法\n",
    "6.1.6\t正则化\n",
    "6.2\t逻辑回归实现\n",
    "6.2.1\tSklearn实现\n",
    "6.2.2\tSpark实现\n",
    "6.2.3\tTensorFlow实现\n",
    "6.2.4\t效果总结"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "逻辑回归是一种解决而分类（0-1）问题的机器学习方法。用于获得某种事物的估计值。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "本次直播使用另一ID的安卓手机开了摄像头，使用MAC OBS共享了屏幕。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "输出y=1的odds函数是由输入wx的线性函数表示的模型，这就是逻辑回归模型。\n",
    "当wx的值越接近于正无穷时，p(y=1|x)的概率值也就越接近于1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型参数估计"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "假设现在有n个相互独立的观测事件集合y=(y1,y2...yn),则一个时间y发生的概率为p(y_i)=p^y_i * (1-p)^(1-y_i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我们的整个样本集是m个独立样本出现的似然函数。所以m个样本出现的概率就是他们各自出现的概率相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<src img='./src/0602.jpeg'/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<src img='./src/0602.jpeg'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "极大似然估计就是使L取最大值时候的θ，可以使用梯度上升法求解。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "也可以乘-1/m ,使用梯度下降法求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "对于逻辑回归的损失函数构成的模型，有时候可能参数权重很大，有些权重很小，这会导致过拟合，模型的复杂度很高，但是泛化能力较差。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "过拟合的解决办法：\n",
    "1、减少特征\n",
    "2、正则化。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在损失函数里加入一个正则化项，正则化项就是权重的L1或者L2范数乘以一个正则系数，用来控制损失函数和正则化项的比重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 防止过拟合的目的是防止最后训练出来的模型过分的依赖某一个特征，当最小化损失函数时，某一维度很大，拟合出来的函数值与真实值之间的距离很小，通过正则化可以使整体的损失值变大，从而避免过分依赖某一维度的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加正则化项的前提是对特征要进行归一化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归实现"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在Sklearn中，逻辑回归的实现方法在model.LogisticRegression。\n",
    "实现方法：\n",
    "class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\" >逻辑回归sklearn函数说明</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\" >逻辑回归sklearn函数说明</a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "penalty：正则惩罚项，str类型，支持'l1'或者'l2'，默认'l2'。\n",
    "dual：对偶参数，dual=True只用在求解线性多核的L2惩罚项上。\n",
    "当样本数量大于样本特征的时候，通常设置dual为False。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tol：停止求解的标准。\n",
    "C：正则化系数的倒数，越小表示越强的正则化。\n",
    "fit_intercept：是否需要常量截距。\n",
    "intercept_scaling：在正则化项为liblinnear且fit_intercept=True时候才有用。\n",
    "class_weight：分类权重设定，可以是一个词典或者balanced字符串，默认不考虑权重。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "第一类问题：误分类代价很大。\n",
    "第二类问题：样本严重失衡。\n",
    "这个时候可以借助class_weight去做设置。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "random_state：随机数种子。\n",
    "max_iter：算法收敛的最大迭代次数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### solver：优化算法的选择参数。\n",
    "a) liblinear：使用了开源的liblinear库实现，内部使用了座标轴下降法来迭代优化损失函数。\n",
    "b) lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。\n",
    "c) newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。\n",
    "d) sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。\n",
    "\n",
    "newton-cg, lbfgs和sag这三种优化算法时都需要损失函数的一阶或者二阶连续导数，因此不能用于没有连续导数的L1正则化，只能用于L2正则化。\n",
    "而liblinear通吃L1正则化和L2正则化。\n",
    "\n",
    "sag每次仅仅使用了部分样本进行梯度迭代，所以当样本量少的时候不要选择它，而如果样本量非常大，比如大于10万，sag是第一选择。\n",
    "但是sag不能用于L1正则化，所以当你有大量的样本，又需要L1正则化的话就要自己做取舍了。要么通过对样本采样来降低样本量，要么回到L2正则化。\n",
    "\n",
    "从上面的描述，大家可能觉得，既然newton-cg, lbfgs和sag这么多限制，如果不是大样本，我们选择liblinear不就行了嘛！错，因为liblinear也有自己的弱点！我们知道，逻辑回归有二元逻辑回归和多元逻辑回归。\n",
    "对于多元逻辑回归常见的有one-vs-rest(OvR)和many-vs-many(MvM)两种。而MvM一般比OvR分类相对准确一些。郁闷的是liblinear只支持OvR，不支持MvM，这样如果我们需要相对精确的多元逻辑回归时，就不能选择liblinear了。\n",
    "也意味着如果我们需要相对精确的多元逻辑回归不能使用L1正则化了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### multi_class参数决定了我们分类方式的选择，有 ovr和multinomial两个值可以选择，默认是 ovr。\n",
    "\n",
    "ovr即前面提到的one-vs-rest(OvR)，而multinomial即前面提到的many-vs-many(MvM)。\n",
    "如果是二元逻辑回归，ovr和multinomial并没有任何区别，区别主要在多元逻辑回归上。\n",
    "\n",
    "OvR的思想很简单，无论你是多少元逻辑回归，我们都可以看做二元逻辑回归。\n",
    "具体做法是，对于第K类的分类决策，我们把所有第K类的样本作为正例，除了第K类样本以外的所有样本都作为负例，然后在上面做二元逻辑回归，得到第K类的分类模型。其他类的分类模型获得以此类推。\n",
    "\n",
    "而MvM则相对复杂，这里举MvM的特例one-vs-one(OvO)作讲解。\n",
    "如果模型有T类，我们每次在所有的T类样本里面选择两类样本出来，不妨记为T1类和T2类，把所有的输出为T1和T2的样本放在一起，把T1作为正例，T2作为负例，进行二元逻辑回归，得到模型参数。\n",
    "我们一共需要T(T-1)/2次分类。\n",
    "\n",
    "从上面的描述可以看出OvR相对简单，但分类效果相对略差（这里指大多数样本分布情况，某些样本分布下OvR可能更好）。而MvM分类相对精确，但是分类速度没有OvR快。\n",
    "\n",
    "如果选择了ovr，则4种损失函数的优化方法liblinear，newton-cg, lbfgs和sag都可以选择。但是如果选择了multinomial,则只能选择newton-cg, lbfgs和sag了。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "verbose：日志冗余长度，0:不输出训练过程。1偶尔输出结果。大于1，对每个子模型都输出。\n",
    "warm_start：热启动参数，True表示下一次训练是以追加树的形式进行的。既重新使用上一次的调用进行初始化。\n",
    "\n",
    "n_jobs：并行数，默认为1，1表示用CPU的一个内核运行程序。2表示用CPU的2个内核运行程序。-1表示用所有CPU内核运行程序。\n",
    "l1_ratio：\n",
    "The Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. \n",
    "是Elastic-Net的参数，介于0-1之间。\n",
    "Only used if penalty='elasticnet'. \n",
    "只有elasticnet的惩罚项下有用。\n",
    "Setting l1_ratio=0 is equivalent to using penalty='l2', while setting l1_ratio=1 is equivalent to using penalty='l1'. \n",
    "当设置为0时候为l2正则，1时候为l1正则。\n",
    "For 0 < l1_ratio <1, the penalty is a combination of L1 and L2.\n",
    "大于0小于1时候为两种正则方式的结合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
